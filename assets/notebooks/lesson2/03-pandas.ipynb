{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9914755e",
   "metadata": {},
   "source": [
    "## 3. Pandas 快速上手：玩转表格数据\n",
    "\n",
    "本章，你将学到：\n",
    "- **核心数据结构**：理解 `Series` (一维序列) 和 `DataFrame` (二维数据框)。\n",
    "- **数据导入与导出**: 轻松读写 `CSV` 和 `Excel` 文件。\n",
    "- **数据筛选与查询**: 掌握 `loc`, `iloc` 和布尔索引，像 SQL 一样查询数据。\n",
    "- **数据清洗**: 处理缺失值、重复值和转换数据类型。\n",
    "- **分组与聚合**: 使用 `groupby` 进行数据分组并计算统计量。\n",
    "- **数据合并与重塑**: 掌握 `merge`, `join`, `concat` 和 `pivot_table`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31361587",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pandas: Python 世界的 Excel\n",
    "\n",
    "如果说 NumPy 是处理数值数组的专家，那么 Pandas 就是处理**表格数据（tabular data）**的王者。在商业分析和数据科学领域，我们遇到的大部分数据都是结构化的二维表格，就像 Excel 表格一样。Pandas 的设计初衷就是让 Python 拥有强大、灵活、易用的数据清洗和分析能力。\n",
    "\n",
    "**为什么 Pandas 如此核心？**\n",
    "\n",
    "1.  **直观的数据结构**: Pandas 的 `DataFrame` 对象与我们熟悉的电子表格或数据库表非常相似，易于理解和操作。\n",
    "2.  **强大的 I/O 功能**: 只需一行代码，就能轻松读取 CSV、Excel、SQL 数据库等多种来源的数据。\n",
    "3.  **丰富的数据操作**: 它提供了海量的方法，用于数据筛选、清洗、转换、合并、重塑等，几乎能满足所有数据预处理需求。\n",
    "4.  **与生态无缝集成**: Pandas 与 NumPy、Matplotlib、Scikit-learn 等库紧密集成，是整个数据科学生态的“数据中枢”。\n",
    "\n",
    "本章将带你掌握 Pandas 最常用、最重要的核心功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aec5f0",
   "metadata": {},
   "source": [
    "> **有用链接**:\n",
    "> - [Pandas 官方网站](https://pandas.pydata.org/)\n",
    "> - [Pandas 官方用户指南](https://pandas.pydata.org/docs/user_guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d186e2",
   "metadata": {},
   "source": [
    "### 准备工作\n",
    "\n",
    "在开始之前，我们需要导入 Pandas 库。业界惯例是将其重命名为 `pd`。同时，我们也会导入 NumPy，因为 Pandas 大量依赖于它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfffd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfbc6f",
   "metadata": {},
   "source": [
    "### 一、核心数据结构：`Series` 与 `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7a271",
   "metadata": {},
   "source": [
    "#### 1. `Series`: 带标签的一维数组\n",
    "`Series` 是一个类似于一维数组的对象，但它有一个额外的**标签（Index）**，可以让我们用标签而不是数字位置来访问元素。你可以把它想象成一个只有一列的 Excel 表格，其中一列是数据，另一列是行索引。\n",
    "\n",
    "> `pd.Series(data=None, index=None, dtype=None, name=None)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `data` | 输入的数据，可以是列表、字典、标量等。 |\n",
    "| `index` | 自定义标签索引，长度需与数据匹配。 |\n",
    "| `dtype` | 指定数据类型，默认会自动推断。 |\n",
    "| `name` | 为 `Series` 命名，便于在结果中识别。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b4a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个简单的 Series:\n",
      "0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "4    50\n",
      "dtype: int64\n",
      "\n",
      "带自定义索引的 Series:\n",
      "a    10\n",
      "b    20\n",
      "c    30\n",
      "dtype: int64\n",
      "\n",
      "获取索引为 'b' 的值: 20\n"
     ]
    }
   ],
   "source": [
    "# 从列表创建一个 Series，Pandas 会自动创建从 0 开始的整数索引\n",
    "s = pd.Series([10, 20, 30, 40, 50])\n",
    "print(\"一个简单的 Series:\")\n",
    "print(s)\n",
    "\n",
    "# 你也可以在创建时自定义索引、数据类型以及名称\n",
    "s_custom_index = pd.Series([10, 20, 30], index=['a', 'b', 'c'], dtype='float64', name='scores')\n",
    "print(\"\\n带自定义索引的 Series (含 dtype 与 name):\")\n",
    "print(s_custom_index)\n",
    "\n",
    "# 通过索引访问数据\n",
    "val = s_custom_index['b']\n",
    "print(f\"\\n获取索引为 'b' 的值: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dce0aa",
   "metadata": {},
   "source": [
    "#### 2. `DataFrame`: 带标签的二维表格\n",
    "`DataFrame` 是 Pandas 的核心，它是一个二维的、大小可变的、异构的表格数据结构，拥有带标签的行（`index`）和列（`columns`）。你可以把它看作一个 Excel 工作表、一个 SQL 表，或者一个由多个 `Series` 共享相同索引组成的字典。\n",
    "\n",
    "> `pd.DataFrame(data=None, index=None, columns=None, dtype=None)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `data` | 数据源（字典、二维数组、Series 字典等）。 |\n",
    "| `index` | 行索引标签，长度需与行数一致。 |\n",
    "| `columns` | 列索引标签，长度需与列数一致。 |\n",
    "| `dtype` | 指定统一的数据类型。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335363ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个简单的 DataFrame:\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n",
      "3    David   40      Houston\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "}\n",
    "df_people = pd.DataFrame(data, index=['R1', 'R2', 'R3', 'R4'])\n",
    "print(\"一个简单的 DataFrame:\")\n",
    "print(df_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4d34f",
   "metadata": {},
   "source": [
    "### 二、数据导入与导出：连接现实世界\n",
    "\n",
    "数据分析的第一步通常是加载数据。Pandas 提供了强大而易用的函数来读取各种格式的文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e32c9",
   "metadata": {},
   "source": [
    "#### 1. 读取 CSV 和 Excel 文件\n",
    "- **`pd.read_csv()`**: 读取逗号分隔值（CSV）文件，这是最常用的数据格式。\n",
    "- **`pd.read_excel()`**: 读取 Excel 文件 (`.xls` 或 `.xlsx`)。注意，使用此功能可能需要额外安装 `openpyxl` 库 (`pip install openpyxl`)。\n",
    "\n",
    "> `pd.read_csv(filepath_or_buffer, sep=',', usecols=None, parse_dates=None, dtype=None, index_col=None)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `filepath_or_buffer` | 文件路径或类文件对象。 |\n",
    "| `sep` | 分隔符，默认逗号，可改为制表符等。 |\n",
    "| `usecols` | 只读取指定列，减少内存使用。 |\n",
    "| `parse_dates` | 指定需要解析为日期的列。 |\n",
    "| `dtype` | 指定列的数据类型，避免自动推断错误。 |\n",
    "| `index_col` | 指定列作为行索引。 |\n",
    "\n",
    "> `pd.read_excel(io, sheet_name=0, usecols=None, dtype=None, engine=None)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `io` | Excel 文件路径或缓冲区。 |\n",
    "| `sheet_name` | 工作表名称或索引，默认第一个。 |\n",
    "| `usecols` | 只读取指定列。 |\n",
    "| `dtype` | 指定列数据类型。 |\n",
    "| `engine` | 指定读写引擎，如 `openpyxl`。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模拟的销售数据 DataFrame:\n",
      "         Date     Category   Product  Sales Region\n",
      "0  2023-01-01  Electronics    Laptop   1200   East\n",
      "1  2023-01-01        Books     Novel     30   West\n",
      "2  2023-01-02  Electronics     Mouse     50   East\n",
      "3  2023-01-02        Books    Poetry     25   West\n",
      "4  2023-01-03  Electronics  Keyboard    150   East\n"
     ]
    }
   ],
   "source": [
    "# 假设我们有一个名为 'sales_data.csv' 的文件\n",
    "# df_sales = pd.read_csv('sales_data.csv', usecols=['Date', 'Sales', 'Region'], parse_dates=['Date'], dtype={'Sales': 'float64'})\n",
    "\n",
    "sales_data = {\n",
    "    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n",
    "    'Category': ['Electronics', 'Books', 'Electronics', 'Books', 'Electronics'],\n",
    "    'Product': ['Laptop', 'Novel', 'Mouse', 'Poetry', 'Keyboard'],\n",
    "    'Sales': [1200, 30, 50, 25, 150],\n",
    "    'Region': ['East', 'West', 'East', 'West', 'East']\n",
    "}\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "print(\"模拟的销售数据 DataFrame:\")\n",
    "print(df_sales)\n",
    "\n",
    "# 读取 Excel 示例\n",
    "# df_excel = pd.read_excel('sales_data.xlsx', sheet_name='January', usecols='A:E')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d929a",
   "metadata": {},
   "source": [
    "#### 2. 快速检视数据\n",
    "加载数据后，我们通常会做一些快速的探索性检查。\n",
    "\n",
    "| 方法 | 常用参数 | 说明 |\n",
    "| --- | --- | --- |\n",
    "| `df.head(n=5)` | `n`: 返回的行数 | 查看前几行，快速了解数据结构。 |\n",
    "| `df.tail(n=5)` | `n`: 返回的行数 | 查看末尾几行是否存在缺失或异常。 |\n",
    "| `df.info(verbose=None, memory_usage=None)` | `memory_usage`: 统计内存占用；`verbose`: 控制输出细节 | 获取列类型、非空计数以及内存使用。 |\n",
    "| `df.describe(percentiles=None, include=None, exclude=None)` | `include`: 指定包含的列类型；`percentiles`: 自定义百分位 | 生成描述性统计，支持数值和类别数据。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c30c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 3 行数据:\n",
      "         Date     Category Product  Sales Region\n",
      "0  2023-01-01  Electronics  Laptop   1200   East\n",
      "1  2023-01-01        Books   Novel     30   West\n",
      "2  2023-01-02  Electronics   Mouse     50   East\n",
      "\n",
      "数据信息摘要:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Date      5 non-null      object\n",
      " 1   Category  5 non-null      object\n",
      " 2   Product   5 non-null      object\n",
      " 3   Sales     5 non-null      int64 \n",
      " 4   Region    5 non-null      object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 332.0+ bytes\n",
      "\n",
      "描述性统计:\n",
      "             Sales\n",
      "count     5.000000\n",
      "mean    291.000000\n",
      "std     510.666232\n",
      "min      25.000000\n",
      "25%      30.000000\n",
      "50%      50.000000\n",
      "75%     150.000000\n",
      "max    1200.000000\n"
     ]
    }
   ],
   "source": [
    "head_view = df_sales.head(n=3)\n",
    "print(\"前 3 行数据:\")\n",
    "print(head_view)\n",
    "\n",
    "tail_view = df_sales.tail(n=2)\n",
    "print(\"\\n最后 2 行数据:\")\n",
    "print(tail_view)\n",
    "\n",
    "print(\"\\n数据信息摘要:\")\n",
    "df_sales.info(memory_usage='deep')\n",
    "\n",
    "desc_stats = df_sales.describe(include='all')\n",
    "print(\"\\n描述性统计:\")\n",
    "print(desc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b86bb9",
   "metadata": {},
   "source": [
    "#### 3. 导出数据\n",
    "分析完成后，你可能需要将处理好的数据保存到新文件中。\n",
    "\n",
    "| 方法 | 常用参数 | 说明 |\n",
    "| --- | --- | --- |\n",
    "| `df.to_csv(path_or_buf, index=False, columns=None, encoding='utf-8', compression=None)` | `index`: 是否写入索引；`columns`: 选择导出的列；`encoding`: 指定编码；`compression`: 压缩格式 | 导出为 CSV 文件或缓冲区。 |\n",
    "| `df.to_excel(excel_writer, sheet_name='Sheet1', index=False, columns=None, engine=None)` | `sheet_name`: 工作表名称；`engine`: 指定写入引擎（如 `openpyxl`）；`columns`: 选择导出的列 | 导出为 Excel 文件或缓冲区。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c99598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.to_csv('sales_export.csv', index=False, columns=['Date', 'Product', 'Sales'])\n",
    "# df_sales.to_excel('sales_export.xlsx', index=False, sheet_name='Sales Summary', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd022264",
   "metadata": {},
   "source": [
    "### 三、选择与过滤：像 SQL 一样查询\n",
    "\n",
    "从庞大的数据集中精确地提取你需要的行和列，是数据分析的核心技能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63428fca",
   "metadata": {},
   "source": [
    "#### 1. 选择列\n",
    "- 使用 `[]`：最简单的方式，类似于字典的 key 索引。\n",
    "- 使用 `.`：如果列名是有效的 Python 标识符（没有空格、特殊字符），可以用点号访问。\n",
    "\n",
    "| 语法 | 返回对象 | 说明 |\n",
    "| --- | --- | --- |\n",
    "| `df['col']` | `Series` | 根据列标签选择单列。 |\n",
    "| `df[['col1', 'col2']]` | `DataFrame` | 使用列表选择多列。 |\n",
    "| `df.col` | `Series` | 语法糖，仅当列名符合标识符规则时可用。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b955c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择 'Sales' 列 (Series):\n",
      "0    1200\n",
      "1      30\n",
      "2      50\n",
      "3      25\n",
      "4     150\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "选择 'Date', 'Product', 'Sales' 三列 (DataFrame):\n",
      "         Date   Product  Sales\n",
      "0  2023-01-01    Laptop   1200\n",
      "1  2023-01-01     Novel     30\n",
      "2  2023-01-02     Mouse     50\n",
      "3  2023-01-02    Poetry     25\n",
      "4  2023-01-03  Keyboard    150\n"
     ]
    }
   ],
   "source": [
    "sales_column = df_sales['Sales']\n",
    "print(\"选择 'Sales' 列 (Series):\")\n",
    "print(sales_column)\n",
    "\n",
    "selected_cols = df_sales[['Date', 'Product', 'Sales']]\n",
    "print(\"\\n选择 'Date', 'Product', 'Sales' 三列 (DataFrame):\")\n",
    "print(selected_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6bf8d",
   "metadata": {},
   "source": [
    "#### 2. 使用 `loc` 和 `iloc` 进行选择\n",
    "- **`df.loc[]`**：基于标签 (Label) 的选择。\n",
    "- **`df.iloc[]`**：基于整数位置 (Integer location) 的选择。\n",
    "\n",
    "> `df.loc[row_indexer, column_indexer]`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `row_indexer` | 行标签、切片、布尔数组或索引器对象。 |\n",
    "| `column_indexer` | 列标签或切片，可选，省略时返回所有列。 |\n",
    "\n",
    "> `df.iloc[row_indexer, column_indexer]`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `row_indexer` | 行的整数位置或切片。 |\n",
    "| `column_indexer` | 列的整数位置或切片，可选。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9056c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 loc 选择第 2 行:\n",
      "Date        2023-01-01\n",
      "Category         Books\n",
      "Product          Novel\n",
      "Sales               30\n",
      "Region            West\n",
      "Name: 1, dtype: object\n",
      "\n",
      "使用 loc 选择特定行和列:\n",
      "  Product  Sales\n",
      "1   Novel     30\n",
      "2   Mouse     50\n",
      "3  Poetry     25\n",
      "\n",
      "使用 iloc 选择第 2 行:\n",
      "Date        2023-01-01\n",
      "Category         Books\n",
      "Product          Novel\n",
      "Sales               30\n",
      "Region            West\n",
      "Name: 1, dtype: object\n",
      "\n",
      "使用 iloc 选择特定行和列:\n",
      "  Product  Sales\n",
      "1   Novel     30\n",
      "2   Mouse     50\n"
     ]
    }
   ],
   "source": [
    "row_1_loc = df_sales.loc[1]\n",
    "print(\"使用 loc 选择第 2 行:\")\n",
    "print(row_1_loc)\n",
    "\n",
    "subset_loc = df_sales.loc[1:3, ['Product', 'Sales']]\n",
    "print(\"\\n使用 loc 选择特定行和列:\")\n",
    "print(subset_loc)\n",
    "\n",
    "row_1_iloc = df_sales.iloc[1]\n",
    "print(\"\\n使用 iloc 选择第 2 行:\")\n",
    "print(row_1_iloc)\n",
    "\n",
    "subset_iloc = df_sales.iloc[1:3, 2:4]\n",
    "print(\"\\n使用 iloc 选择特定行和列:\")\n",
    "print(subset_iloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e0a4e",
   "metadata": {},
   "source": [
    "#### 3. 布尔索引 (Conditional Filtering)\n",
    "布尔索引允许你根据一个或多个条件来过滤行，是最常用的筛选方式之一。\n",
    "\n",
    "| 方法 | 常用参数 | 说明 |\n",
    "| --- | --- | --- |\n",
    "| `Series > value` | 任意比较运算符 | 基于阈值的筛选。 |\n",
    "| `(cond1) & (cond2)` | `&`, `|`, `~` | 组合多个布尔条件时需要括号。 |\n",
    "| `Series.isin(values)` | `values`: 可迭代对象 | 判断元素是否属于给定集合。 |\n",
    "| `Series.between(left, right, inclusive='both')` | `left`, `right`, `inclusive` | 按区间筛选数值范围。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d5b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "销售额大于 100 的记录:\n",
      "         Date     Category   Product  Sales Region\n",
      "0  2023-01-01  Electronics    Laptop   1200   East\n",
      "4  2023-01-03  Electronics  Keyboard    150   East\n",
      "\n",
      "East 地区且销售额大于 100 的记录:\n",
      "         Date     Category   Product  Sales Region\n",
      "0  2023-01-01  Electronics    Laptop   1200   East\n",
      "4  2023-01-03  Electronics  Keyboard    150   East\n",
      "\n",
      "类别为 'Books' 或 'Clothing' 的记录:\n",
      "         Date Category Product  Sales Region\n",
      "1  2023-01-01    Books   Novel     30   West\n",
      "3  2023-01-02    Books  Poetry     25   West\n"
     ]
    }
   ],
   "source": [
    "high_sales_mask = df_sales['Sales'] > 100\n",
    "high_sales_df = df_sales[high_sales_mask]\n",
    "print(\"销售额大于 100 的记录:\")\n",
    "print(high_sales_df)\n",
    "\n",
    "east_high_sales_mask = (df_sales['Region'] == 'East') & (df_sales['Sales'] > 100)\n",
    "east_high_sales_df = df_sales[east_high_sales_mask]\n",
    "print(\"\\nEast 地区且销售额大于 100 的记录:\")\n",
    "print(east_high_sales_df)\n",
    "\n",
    "books_or_clothing_mask = df_sales['Category'].isin(['Books', 'Clothing'])\n",
    "books_or_clothing_df = df_sales[books_or_clothing_mask]\n",
    "print(\"\\n类别为 'Books' 或 'Clothing' 的记录:\")\n",
    "print(books_or_clothing_df)\n",
    "\n",
    "mid_sales_mask = df_sales['Sales'].between(left=20, right=200, inclusive='both')\n",
    "mid_sales_df = df_sales[mid_sales_mask]\n",
    "print(\"\\n销售额在 20 到 200 之间的记录:\")\n",
    "print(mid_sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d724aef",
   "metadata": {},
   "source": [
    "### 四、数据清洗：让数据变得可用\n",
    "\n",
    "真实世界的数据很少是完美的，通常充满了缺失值、重复值或错误的数据类型。数据清洗是数据分析流程中至关重要的一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c171d0",
   "metadata": {},
   "source": [
    "#### 1. 处理缺失值\n",
    "首先，我们需要创建一个带缺失值（`np.nan`）的 DataFrame。\n",
    "\n",
    "> `df.dropna(axis=0, how='any', subset=None, inplace=False)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `axis` | 沿哪个轴删除缺失值（0=行，1=列）。 |\n",
    "| `how` | `'any'` 表示任一缺失即删除，`'all'` 表示整行/列全缺失才删除。 |\n",
    "| `subset` | 只在指定列上检查缺失。 |\n",
    "| `inplace` | 是否直接修改原对象。 |\n",
    "\n",
    "> `df.fillna(value=None, method=None, axis=None, inplace=False, limit=None)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `value` | 用于填充的标量、字典或 Series。 |\n",
    "| `method` | 使用 `'ffill'` 或 `'bfill'` 沿轴方向填充。 |\n",
    "| `axis` | 指定沿行或列填充。 |\n",
    "| `limit` | 限制连续填充的次数。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "带缺失值的 DataFrame:\n",
      "     A    B   C\n",
      "0  1.0  5.0  10\n",
      "1  2.0  NaN  20\n",
      "2  NaN  NaN  30\n",
      "3  4.0  8.0  40\n",
      "\n",
      "每列的缺失值数量:\n",
      "A    1\n",
      "B    2\n",
      "C    0\n",
      "dtype: int64\n",
      "\n",
      "删除所有含缺失值的行后:\n",
      "     A    B   C\n",
      "0  1.0  5.0  10\n",
      "3  4.0  8.0  40\n",
      "\n",
      "用 0 填充所有缺失值后:\n",
      "     A    B   C\n",
      "0  1.0  5.0  10\n",
      "1  2.0  0.0  20\n",
      "2  0.0  0.0  30\n",
      "3  4.0  8.0  40\n",
      "\n",
      "用列均值填充缺失值后:\n",
      "          A    B   C\n",
      "0  1.000000  5.0  10\n",
      "1  2.000000  6.5  20\n",
      "2  2.333333  6.5  30\n",
      "3  4.000000  8.0  40\n"
     ]
    }
   ],
   "source": [
    "data_with_nan = {\n",
    "    'A': [1, 2, np.nan, 4],\n",
    "    'B': [5, np.nan, np.nan, 8],\n",
    "    'C': [10, 20, 30, 40]\n",
    "}\n",
    "df_nan = pd.DataFrame(data_with_nan)\n",
    "print(\"带缺失值的 DataFrame:\")\n",
    "print(df_nan)\n",
    "\n",
    "missing_counts = df_nan.isnull().sum()\n",
    "print(\"\\n每列的缺失值数量:\")\n",
    "print(missing_counts)\n",
    "\n",
    "df_dropped = df_nan.dropna(axis=0, how='any')\n",
    "print(\"\\n删除所有含缺失值的行后:\")\n",
    "print(df_dropped)\n",
    "\n",
    "df_filled_zero = df_nan.fillna(value=0)\n",
    "print(\"\\n用 0 填充所有缺失值后:\")\n",
    "print(df_filled_zero)\n",
    "\n",
    "df_filled_mean = df_nan.fillna(value={\n",
    "    'A': df_nan['A'].mean(),\n",
    "    'B': df_nan['B'].mean()\n",
    "})\n",
    "print(\"\\n用列均值填充缺失值后:\")\n",
    "print(df_filled_mean)\n",
    "\n",
    "df_filled_ffill = df_nan.fillna(method='ffill', limit=1, axis=0)\n",
    "print(\"\\n前向填充（limit=1）后的结果:\")\n",
    "print(df_filled_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e93731",
   "metadata": {},
   "source": [
    "#### 2. 处理重复值\n",
    "\n",
    "> `df.duplicated(subset=None, keep='first')`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `subset` | 指定用于识别重复的列集合。 |\n",
    "| `keep` | `'first'` 保留首次出现，`'last'` 保留最后一次，`False` 标记所有重复。 |\n",
    "\n",
    "> `df.drop_duplicates(subset=None, keep='first', inplace=False)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `subset` | 与 `duplicated` 相同，控制比较的列。 |\n",
    "| `keep` | 控制保留哪一个重复值。 |\n",
    "| `inplace` | 是否就地修改。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bd92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "带重复值的 DataFrame:\n",
      "   ID Name\n",
      "0   1    A\n",
      "1   2    B\n",
      "2   2    B\n",
      "3   3    C\n",
      "\n",
      "检查重复行:\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "dtype: bool\n",
      "\n",
      "删除重复行后:\n",
      "   ID Name\n",
      "0   1    A\n",
      "1   2    B\n",
      "3   3    C\n"
     ]
    }
   ],
   "source": [
    "data_with_dups = {\n",
    "    'ID': [1, 2, 2, 3],\n",
    "    'Name': ['A', 'B', 'B', 'C']\n",
    "}\n",
    "df_dups = pd.DataFrame(data_with_dups)\n",
    "print(\"带重复值的 DataFrame:\")\n",
    "print(df_dups)\n",
    "\n",
    "is_duplicate = df_dups.duplicated(subset=['ID', 'Name'], keep='first')\n",
    "print(\"\\n检查重复行:\")\n",
    "print(is_duplicate)\n",
    "\n",
    "df_no_dups = df_dups.drop_duplicates(subset=['ID', 'Name'], keep='first')\n",
    "print(\"\\n删除重复行后:\")\n",
    "print(df_no_dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4717335d",
   "metadata": {},
   "source": [
    "#### 3. 数据类型转换\n",
    "有时，数字被错误地存储为字符串，或者日期被存为普通文本。我们需要将它们转换为正确的类型才能进行计算或分析。\n",
    "\n",
    "> `Series.astype(dtype, copy=True, errors='raise')`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `dtype` | 目标数据类型（如 `float`, `int`, `str`）。 |\n",
    "| `copy` | 是否返回数据的副本。 |\n",
    "| `errors` | `'raise'` 遇到无效转换时报错，`'ignore'` 则保持原值。 |\n",
    "\n",
    "> `pd.to_datetime(arg, format=None, errors='raise', infer_datetime_format=False)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `arg` | 需要转换的序列、列表或单个日期字符串。 |\n",
    "| `format` | 指定日期格式，能显著提升解析速度。 |\n",
    "| `errors` | `'raise'`、`'coerce'` 或 `'ignore'`，控制解析失败的处理方式。 |\n",
    "| `infer_datetime_format` | 自动推断格式，提高性能。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sales' 列类型错误的信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Date      5 non-null      object\n",
      " 1   Category  5 non-null      object\n",
      " 2   Product   5 non-null      object\n",
      " 3   Sales     5 non-null      object\n",
      " 4   Region    5 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 332.0+ bytes\n",
      "\n",
      "'Sales' 列类型修正后的信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Date      5 non-null      object \n",
      " 1   Category  5 non-null      object \n",
      " 2   Product   5 non-null      object \n",
      " 3   Sales     5 non-null      float64\n",
      " 4   Region    5 non-null      object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 332.0+ bytes\n",
      "\n",
      "'Date' 列类型修正后的信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Date      5 non-null      datetime64[ns]\n",
      " 1   Category  5 non-null      object        \n",
      " 2   Product   5 non-null      object        \n",
      " 3   Sales     5 non-null      float64       \n",
      " 4   Region    5 non-null      object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(3)\n",
      "memory usage: 332.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_wrong_type = df_sales.copy()\n",
    "df_wrong_type['Sales'] = df_wrong_type['Sales'].astype(str)\n",
    "print(\"'Sales' 列类型错误的信息:\")\n",
    "df_wrong_type.info()\n",
    "\n",
    "df_corrected_type = df_wrong_type.copy()\n",
    "df_corrected_type['Sales'] = df_corrected_type['Sales'].astype(dtype='float64', errors='raise')\n",
    "print(\"\\n'Sales' 列类型修正后的信息:\")\n",
    "df_corrected_type.info()\n",
    "\n",
    "df_corrected_type['Date'] = pd.to_datetime(df_corrected_type['Date'], format='%Y-%m-%d', errors='raise')\n",
    "print(\"\\n'Date' 列类型修正后的信息:\")\n",
    "df_corrected_type.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a3349",
   "metadata": {},
   "source": [
    "### 五、分组与聚合：从细节中看全局\n",
    "\n",
    "`groupby` 操作遵循“拆分-应用-合并 (Split-Apply-Combine)”模式，是数据分析的基石。\n",
    "\n",
    "> `df.groupby(by, axis=0, sort=True, as_index=True, dropna=True)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `by` | 用于分组的列标签、列表或字典。 |\n",
    "| `axis` | 默认为行分组 (`axis=0`)，也可按列分组。 |\n",
    "| `sort` | 是否对分组键排序。 |\n",
    "| `as_index` | 为 `True` 时分组键成为结果的索引。 |\n",
    "| `dropna` | 是否忽略分组键为 NA 的行。 |\n",
    "\n",
    "> `GroupBy.agg(func=None, *args, **kwargs)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `func` | 可以是单个函数、函数列表或列-函数映射。 |\n",
    "| `kwargs` | 向聚合函数传递的额外参数。 |\n",
    "| `observed` | 针对分类分组键控制是否仅保留出现过的组合。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按类别分组聚合的结果:\n",
      "              sum        mean  count\n",
      "Category                            \n",
      "Books          55   27.500000      2\n",
      "Electronics  1400  466.666667      3\n",
      "\n",
      "链式调用并重置索引的结果:\n",
      "      Category   sum        mean\n",
      "0        Books    55   27.500000\n",
      "1  Electronics  1400  466.666667\n",
      "\n",
      "按多个列分组聚合的结果:\n",
      "  Region     Category  Sales\n",
      "0   East  Electronics   1400\n",
      "1   West        Books     55\n"
     ]
    }
   ],
   "source": [
    "grouped_by_category = df_sales.groupby(by='Category', sort=True, dropna=False)\n",
    "agg_results = grouped_by_category['Sales'].agg(['sum', 'mean', 'count'])\n",
    "print(\"按类别分组聚合的结果:\")\n",
    "print(agg_results)\n",
    "\n",
    "agg_results_chained = df_sales.groupby('Category')['Sales'].agg(['sum', 'mean']).reset_index()\n",
    "print(\"\\n链式调用并重置索引的结果:\")\n",
    "print(agg_results_chained)\n",
    "\n",
    "agg_multi_group = df_sales.groupby(['Region', 'Category'])['Sales'].sum().reset_index()\n",
    "print(\"\\n按多个列分组聚合的结果:\")\n",
    "print(agg_multi_group)\n",
    "\n",
    "agg_named = df_sales.groupby('Region').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    average_sale=('Sales', 'mean'),\n",
    "    orders=('Sales', 'count')\n",
    ").reset_index()\n",
    "print(\"\\n命名聚合的结果:\")\n",
    "print(agg_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb5568",
   "metadata": {},
   "source": [
    "### 六、合并与重塑：整合多个数据源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfd44e",
   "metadata": {},
   "source": [
    "#### 1. `pd.concat`: 堆叠数据\n",
    "`concat` 用于沿着一个轴将多个 DataFrame 对象堆叠在一起。\n",
    "\n",
    "> `pd.concat(objs, axis=0, join='outer', ignore_index=False, keys=None)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `objs` | 要连接的对象列表或字典。 |\n",
    "| `axis` | 0 表示纵向堆叠，1 表示横向拼接。 |\n",
    "| `join` | `'outer'` 取并集，`'inner'` 取交集。 |\n",
    "| `ignore_index` | 是否重新生成连续索引。 |\n",
    "| `keys` | 为拼接结果添加分层索引。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垂直拼接:\n",
      "    A   B\n",
      "0  A0  B0\n",
      "1  A1  B1\n",
      "0  A2  B2\n",
      "1  A3  B3\n",
      "\n",
      "水平拼接:\n",
      "    A   B   A   B\n",
      "0  A0  B0  A2  B2\n",
      "1  A1  B1  A3  B3\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})\n",
    "df2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']})\n",
    "vertical_concat = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "print(\"垂直拼接:\")\n",
    "print(vertical_concat)\n",
    "\n",
    "horizontal_concat = pd.concat([df1, df2], axis=1, keys=['left', 'right'])\n",
    "print(\"\\n水平拼接:\")\n",
    "print(horizontal_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd5865",
   "metadata": {},
   "source": [
    "#### 2. `pd.merge`: SQL 式连接\n",
    "`merge` 用于根据一个或多个共同的键（列）将不同的 DataFrame 连接起来，类似于 SQL 的 `JOIN`。\n",
    "\n",
    "> `pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, suffixes=('_x', '_y'))\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `left`, `right` | 参与合并的 DataFrame。 |\n",
    "| `how` | 连接方式：`'inner'`, `'left'`, `'right'`, `'outer'`, `'cross'`。 |\n",
    "| `on` | 连接键列名（双方一致时）。 |\n",
    "| `left_on`, `right_on` | 当连接列名不同时时分别指定。 |\n",
    "| `suffixes` | 合并后重复列名的后缀。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f84ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内连接结果:\n",
      "  key   A   B\n",
      "0  K0  A0  B0\n",
      "1  K1  A1  B1\n",
      "\n",
      "左连接结果:\n",
      "  key   A    B\n",
      "0  K0  A0   B0\n",
      "1  K1  A1   B1\n",
      "2  K2  A2  NaN\n"
     ]
    }
   ],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'A': ['A0', 'A1', 'A2']})\n",
    "right = pd.DataFrame({'key': ['K0', 'K1', 'K3'], 'B': ['B0', 'B1', 'B3']})\n",
    "inner_merge = pd.merge(left, right, on='key', how='inner', suffixes=('_left', '_right'), indicator=True)\n",
    "print(\"内连接结果:\")\n",
    "print(inner_merge)\n",
    "\n",
    "left_merge = pd.merge(left, right, on='key', how='left', validate='one_to_one')\n",
    "print(\"\\n左连接结果:\")\n",
    "print(left_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acf7dd",
   "metadata": {},
   "source": [
    "#### 3. `pivot_table`: 数据透视表\n",
    "数据透视表是一种强大的数据重塑和汇总工具，可以将长格式的数据转换为宽格式，非常适合制作交叉分析报告。\n",
    "\n",
    "> `pd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False)`\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| --- | --- |\n",
    "| `values` | 需要聚合的列。 |\n",
    "| `index` | 结果表的行索引。 |\n",
    "| `columns` | 结果表的列索引。 |\n",
    "| `aggfunc` | 聚合函数，可为函数或列表。 |\n",
    "| `fill_value` | 用于填充缺失值。 |\n",
    "| `margins` | 是否添加合计行/列。 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584fa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "销售数据透视表:\n",
      "Category  Books  Electronics\n",
      "Region                      \n",
      "East          0         1400\n",
      "West         55            0\n"
     ]
    }
   ],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    df_sales,\n",
    "    values='Sales',\n",
    "    index='Region',\n",
    "    columns='Category',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    " )\n",
    "print(\"销售数据透视表:\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d0e7e",
   "metadata": {},
   "source": [
    "---\n",
    "**实践小结**: 你已经掌握了使用 Pandas 进行数据导入、检视、筛选、清洗、聚合和合并的核心技能。这些操作构成了绝大多数数据分析项目的骨架。请务必花时间练习 `loc`/`iloc` 的区别、布尔索引的逻辑以及 `groupby` 的思维模式。下一章，我们将学习如何将这些处理好的数据通过 **Matplotlib** 进行可视化呈现。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
